{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Neural Networks for Small Molecule Regression\n",
    "_by David Holmberg and Jonathan Alvarsson (September 2023)_\n",
    "#### Dataset\n",
    "For this exercise we will use a dataset of aqueous solubility for 1142 diverse chemical compounds.\n",
    "\n",
    "#### Modelling comparisons\n",
    "We will be doing a little bit of comparison between classical machine learning methods *Support Vectort Machines* and *Random Forest* to those of *Graph Neural Networks*\n",
    "\n",
    "#### Aims\n",
    "* Introduce the concept of Graph Neural Networks\n",
    "* Introduce PyTorch code for GNNs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load packages\n",
    "First we load the packags that we are going to be using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "# Pytorch and Pytorch Geometric\n",
    "import torch as tch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch_geometric.nn import GCNConv, GATConv, summary as gsummary, global_mean_pool, global_max_pool\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.utils import dropout_adj\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "\n",
    "\n",
    "# Helper libraries\n",
    "from torchsummary import summary as asummary\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import os\n",
    "\n",
    "\n",
    "device = tch.device(\"cuda\" if tch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "Next we define som utility functions that will be used in the lab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(train_losses, val_losses, model_name):\n",
    "    fig = plt.figure(figsize=(15, 5), facecolor='w')\n",
    "    ax = fig.add_subplot(121)\n",
    "    ax.plot(train_losses)\n",
    "    ax.plot(val_losses)\n",
    "    ax.set(title=model_name + ': Model loss', ylabel='Loss', xlabel='Epoch')\n",
    "    ax.legend(['Train', 'Test'], loc='upper right')\n",
    "    ax = fig.add_subplot(122)\n",
    "    ax.plot(np.log(train_losses))\n",
    "    ax.plot(np.log(val_losses))\n",
    "    ax.set(title=model_name + ': Log model loss', ylabel='Log loss', xlabel='Epoch')\n",
    "    ax.legend(['Train', 'Test'], loc='upper right')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "#Set morgan to 3 or 4 and nBits to 1024\n",
    "def smiles_to_fingerprint(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    fp = AllChem.GetMorganFingerprintAsBitVect(mol, 3, nBits=1024)\n",
    "    return list(fp.ToBitString())\n",
    "\n",
    "def smiles_to_mol(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    # if mol is not None:\n",
    "    #     mol = Chem.AddHs(mol)\n",
    "    return mol\n",
    "\n",
    "def read_smiles_data(path_data):\n",
    "    df = pd.read_csv(path_data, sep=',')\n",
    "    df['fingerprint'] = df['SMILES'].apply(smiles_to_fingerprint)\n",
    "    df['fingerprint'] = df['fingerprint'].apply(lambda x: [int(bit) for bit in x])\n",
    "    df['fingerprint'] = df['fingerprint'].apply(lambda x: np.array(x))\n",
    "    df['mol'] = df['SMILES'].apply(smiles_to_mol)\n",
    "    return df\n",
    "\n",
    "def is_hydrogen_donor(atomic_num, hybridization):\n",
    "    return int((atomic_num == 8 or atomic_num == 7) and (hybridization == 3 or hybridization == 2))\n",
    "\n",
    "def is_polar_bond(atom1_num, atom2_num, electronegativity):\n",
    "    en1 = electronegativity.get(atom1_num, None)\n",
    "    en2 = electronegativity.get(atom2_num, None)\n",
    "    if en1 is None or en2 is None:\n",
    "        return 0  # Unknown electronegativity, consider as non-polar\n",
    "    return int(abs(en1 - en2) > 0.4)\n",
    "\n",
    "def electroneg():\n",
    "    return {\n",
    "    1: 2.20,  # H\n",
    "    3: 0.98,  # Li\n",
    "    4: 1.57,  # Be\n",
    "    5: 2.04,  # B\n",
    "    6: 2.55,  # C\n",
    "    7: 3.04,  # N\n",
    "    8: 3.44,  # O\n",
    "    9: 3.98,  # F\n",
    "    11: 0.93, # Na\n",
    "    12: 1.31, # Mg\n",
    "    13: 1.61, # Al\n",
    "    14: 1.90, # Si\n",
    "    15: 2.19, # P\n",
    "    16: 2.58, # S\n",
    "    17: 3.16, # Cl\n",
    "    19: 0.82, # K\n",
    "    20: 1.00, # Ca\n",
    "    22: 1.54, # Ti\n",
    "    24: 1.66, # Cr\n",
    "    25: 1.55, # Mn\n",
    "    26: 1.83, # Fe\n",
    "    27: 1.88, # Co\n",
    "    28: 1.91, # Ni\n",
    "    29: 1.90, # Cu\n",
    "    30: 1.65, # Zn\n",
    "    35: 2.96, # Br\n",
    "    53: 2.66, # I\n",
    "}\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, (GCNConv, GATConv)):\n",
    "        nn.init.xavier_uniform_(m.weight.data)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight.data)\n",
    "\n",
    "def make_pyg(row):\n",
    "    # Create node features\n",
    "    mol = row['mol']\n",
    "    # pauling = electroneg()\n",
    "    atom_num = [atom.GetAtomicNum() for atom in mol.GetAtoms()]\n",
    "    atom_hyb = [int(atom.GetHybridization()) for atom in mol.GetAtoms()]\n",
    "    atom_deg = [atom.GetDegree() for atom in mol.GetAtoms()]\n",
    "    atom_arom = [int(atom.GetIsAromatic()) for atom in mol.GetAtoms()]  # Aromaticity\n",
    "    atom_hydrogens = [atom.GetTotalNumHs() for atom in mol.GetAtoms()]  # Number of hydrogens\n",
    "    # atom_h_donor = [is_hydrogen_donor(num, hyb) for num, hyb in zip(atom_num, atom_hyb)]\n",
    "    atom_charge = [atom.GetFormalCharge() for atom in mol.GetAtoms()]  # Formal charge\n",
    "    atom_chiral_tag = [int(atom.GetChiralTag()) for atom in mol.GetAtoms()]  # Chirality\n",
    "    atom_val = [atom.GetExplicitValence() for atom in mol.GetAtoms()]\n",
    "    #atom_mass = [atom.GetMass() for atom in mol.GetAtoms()]\n",
    "    #atom_pauling = [pauling.get(num, 0) for num in atom_num]\n",
    "    \n",
    "    x1 = tch.tensor(atom_num, dtype=tch.float).view(-1, 1)\n",
    "    x2 = tch.tensor(atom_hyb, dtype=tch.float).view(-1, 1)\n",
    "    x3 = tch.tensor(atom_deg, dtype=tch.float).view(-1, 1)\n",
    "    x4 = tch.tensor(atom_arom, dtype=tch.float).view(-1, 1)\n",
    "    x5 = tch.tensor(atom_hydrogens, dtype=tch.float).view(-1, 1)\n",
    "    x6 = tch.tensor(atom_charge, dtype=tch.float).view(-1, 1)\n",
    "    x7 = tch.tensor(atom_chiral_tag, dtype=tch.float).view(-1, 1)\n",
    "    x8 = tch.tensor(atom_val, dtype=tch.float).view(-1, 1)\n",
    "    # x9 = tch.tensor(atom_h_donor, dtype=tch.float).view(-1, 1)\n",
    "    #x10 = tch.tensor(atom_mass, dtype=tch.float).view(-1, 1)\n",
    "    #x11 = tch.tensor(atom_pauling, dtype=tch.float).view(-1, 1)\n",
    "    \n",
    "    y = tch.tensor(row['measured.log.solubility.mol.L.'], dtype=tch.float).view(-1, 1)\n",
    "    x = tch.cat([x1\n",
    "                 , x2\n",
    "                 , x3\n",
    "                 , x4\n",
    "                 , x5\n",
    "                 , x6\n",
    "                 , x7\n",
    "                 , x8\n",
    "                # , x9\n",
    "                # , x10\n",
    "                 #, x11\n",
    "                 ], dim=1)\n",
    "    \n",
    "    # Create edge features (connectivity)\n",
    "    edge_indices = []\n",
    "    edge_features = []\n",
    "    \n",
    "    for bond in mol.GetBonds():\n",
    "        i = bond.GetBeginAtomIdx()\n",
    "        j = bond.GetEndAtomIdx()\n",
    "        edge_indices.append((i, j))\n",
    "        bond_type = bond.GetBondTypeAsDouble()\n",
    "        is_conjugated = int(bond.GetIsConjugated())  # Conjugation\n",
    "        is_in_ring = int(bond.IsInRing())  # Ring membership\n",
    "        bond_stereo = int(bond.GetStereo())  # Stereo configuration\n",
    "        #bond_polarity = is_polar_bond(atom_num[i], atom_num[j], pauling)\n",
    "\n",
    "        edge_features.append([bond_type\n",
    "                              , is_conjugated\n",
    "                              , is_in_ring\n",
    "                              , bond_stereo\n",
    "                              #, bond_polarity\n",
    "                              ])\n",
    "    \n",
    "    edge_index = tch.tensor(edge_indices, dtype=tch.long).t().contiguous()\n",
    "    edge_attr = tch.tensor(edge_features, dtype=tch.float)\n",
    "    \n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y)\n",
    "\n",
    "    data.smiles = row['SMILES']\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Functions for Neural Networks\n",
    "This cell defines the training methods for the neural networks you will use later in this notebook. Here we can see what parameters are used when training the models, we have:\n",
    " * gnn1_model -- the model that we are going to train\n",
    " * t_loader -- PyTorch specific helper for loading training data\n",
    " * v_loader -- PyTorch specific helper for loading test (validation) data\n",
    " * num_epochs -- the number of epochs to train. The higher the number the longer we will train\n",
    " * batch_size -- training is done in batches. Higher number means faster training, lower number can give higher precission\n",
    " * optimizer -- the optimization algorithm to be used. These days often som variant of Adam is used which also controls the learning rate automaticly during runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitGNN(gnn1_model, t_loader, v_loader, num_epochs, batch_size, optimizer, criterion):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    pbar = tqdm(range(num_epochs), desc=\"Epochs\")\n",
    "    pbar.reset()\n",
    "    pbar_t = tqdm(total=len(t_loader), desc=\"Training Batch:\", leave=False)\n",
    "    pbar_v = tqdm(total=len(v_loader), desc=\"validation Batch:\", leave=False)\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training Phase\n",
    "        gnn1_model.train()\n",
    "        train_loss_items = []\n",
    "        pbar_t.reset()\n",
    "        pbar_v.reset()\n",
    "        for batch in t_loader:\n",
    "            batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            # Use Batch Data object in forward pass\n",
    "            outputs = gnn1_model(batch.x.float(), batch.edge_index, batch.batch)\n",
    "            loss = criterion(outputs, batch.y)\n",
    "\n",
    "            l1_lambda = 0.0001\n",
    "            l1_norm = sum(p.abs().sum() for p in gnn1_model.parameters())\n",
    "            loss += l1_lambda * l1_norm\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss_items.append(loss.item())\n",
    "            pbar_t.update()\n",
    "        avg_train_loss = sum(train_loss_items) / len(train_loss_items)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        # Validation Phase (assuming you have a separate validation loader)\n",
    "        gnn1_model.eval()\n",
    "        val_loss_items = []\n",
    "        with tch.no_grad():\n",
    "            for val_batch in v_loader:\n",
    "                val_batch.to(device)\n",
    "                val_outputs = gnn1_model(val_batch.x.float(), val_batch.edge_index, val_batch.batch)\n",
    "                val_loss = criterion(val_outputs, val_batch.y)\n",
    "                val_loss_items.append(val_loss.item())\n",
    "                pbar_v.update()\n",
    "\n",
    "        avg_val_loss = sum(val_loss_items) / len(val_loss_items)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        # print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {train_losses[-1]:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "        pbar.update(1)\n",
    "        pbar.set_postfix({\"Training Loss\": avg_train_loss, \"Validation Loss\": avg_val_loss})\n",
    "    return gnn1_model, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and preprocess data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and check shape of X and y\n",
    "We load our data from the file solubility.csv and take a quick look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_smiles_data('data/solubility.csv')\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning and housekeeping\n",
    "These lines remove an empty row from the dataset and creates a PyG Graph object for the GNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pyg = df.apply(make_pyg, axis=1)\n",
    "data_pyg = data_pyg[data_pyg.apply(lambda x: len(x.edge_index.shape) != 1)]\n",
    "data_pyg.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split into training and test sets and standardize the data\n",
    "We will make a simple train / test set split. Better might be to do cross validation but as it would also take longer time to run, we will not be doing it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_df = int(len(df) * 0.7)\n",
    "df_train = df.iloc[:n_df]\n",
    "# df_train = df.sample(frac=0.8)\n",
    "df_test = df.drop(df_train.index)\n",
    "\n",
    "df_train.reset_index(drop=True)\n",
    "df_test.reset_index(drop=True)\n",
    "X_train, y_train = df_train['fingerprint'].tolist(), df_train['measured.log.solubility.mol.L.'].tolist()\n",
    "X_test, y_test = df_test['fingerprint'].tolist(), df_test['measured.log.solubility.mol.L.'].tolist()\n",
    "\n",
    "n_train = int(len(data_pyg) * 0.7) # 70% of data for training and 30% for testing\n",
    "indices = np.arange(n_train)\n",
    "data_train = data_pyg[indices[:n_train]]\n",
    "data_train.reset_index(drop=True, inplace=True)\n",
    "data_test = data_pyg[~data_pyg.isin(data_train)]\n",
    "data_test.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Regressor & Support Vector Regressor\n",
    "For comparative purposes we will build a Random Forest and a Support Vector model. For these machine learning algorithms we will just use the default hyper parameter settings, which are often a good place to start. This means that we will just have ampty () after the model definition. To change the hyper parameters from the defaults one needs to specify them within the braces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_model = RandomForestRegressor()\n",
    "RF_model.fit(X_train, y_train)\n",
    "RF_pred = RF_model.predict(X_test)\n",
    "RF_mse = mean_squared_error(y_test, RF_pred)\n",
    "print('Random Forest Regressor: MSE = ' + str(np.round(RF_mse, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SV_model = SVR()\n",
    "SV_model.fit(X_train, y_train)\n",
    "SV_pred = SV_model.predict(X_test)\n",
    "SV_mse = mean_squared_error(y_test, SV_pred)\n",
    "print('Support Vector Regressor: MSE = ' + str(np.round(SV_mse, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing GNNs\n",
    "So, now we have tested regression on molecular descriptors with the two classical machine learning algorithms random forest and support vector machines. Another option that is gaining traction in the research world is using Graph Neural Networks. We will be using an extension library called Pytorch.Geometric for this. There are two cannonical implementations for GNNs - one being Graph Convolutional Networks and the other being Graph Attention Networks. We will try both here. Below you will find two pre-implemented GNNs, one of each type. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define some training parameters that will be shared among both models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = data_train.iloc[5].x.size(1)\n",
    "print('Input Dimensions: ', input_dim)\n",
    "#Loss, Epochs, Batch-size\n",
    "num_epochs = 600\n",
    "batch_size = 64\n",
    "weight_decay = 1e-4\n",
    "criterion = nn.MSELoss()\n",
    "#Data Loaders to handle the graphs we made earlier\n",
    "t_loader = DataLoader(data_train, batch_size=batch_size, shuffle=True)\n",
    "v_loader = DataLoader(data_test, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the actual GNN layers. First we will do the convolutional GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN_conv(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(GNN_conv, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, 32)\n",
    "        self.conv2 = GCNConv(32, 64)\n",
    "        self.conv3 = GCNConv(64, 16)\n",
    "        self.fc3 = nn.Linear(16, 1)  # Output layer with 1 node\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.relu(self.conv1(x, edge_index))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.conv2(x, edge_index))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.conv3(x, edge_index))\n",
    "        x = self.dropout(x)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we train it: (This step takes a little while to run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn_conv_model = GNN_conv(input_dim)\n",
    "gnn_conv_model = gnn_conv_model.to(device)\n",
    "# Adam adjusts learning rate as needed\n",
    "optimizer = optim.AdamW(gnn_conv_model.parameters(), lr=0.0001,weight_decay= weight_decay)\n",
    "#Traing the Model\n",
    "gnn_conv_model, train_losses, val_losses = fitGNN(gnn_conv_model, t_loader, v_loader, num_epochs, batch_size, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally we make a plot showing how the training has progressed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn_conv_model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with tch.no_grad():\n",
    "    for v_batch in v_loader:\n",
    "        v_batch.to(device)\n",
    "        test_outputs = gnn_conv_model(v_batch.x, v_batch.edge_index, v_batch.batch)\n",
    "        all_preds.extend(test_outputs.tolist())\n",
    "        all_labels.extend(v_batch.y.tolist())\n",
    "all_preds_tensor = tch.tensor(all_preds)\n",
    "all_labels_tensor = tch.tensor(all_labels)\n",
    "gnn_conv_mse = mean_squared_error(all_labels_tensor, all_preds_tensor)\n",
    "\n",
    "print(f'Convolutional GNN Regression: MSE = {gnn_conv_mse:.3f}')\n",
    "plot_history(train_losses, val_losses, 'Convolutional GNN Regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will do the attention based GNN. First we define the layers of the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GNN_atten(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(GNN_atten, self).__init__()\n",
    "        self.conv1 = GATConv(input_dim, 32, heads=1, concat=True)  # Two attention heads\n",
    "        self.conv2 = GATConv(32, 32, heads=2, concat=True)  # Two attention heads\n",
    "        self.conv3 = GATConv(64, 8, heads=2, concat=True)  # Two attention heads\n",
    "        self.fc3 = nn.Linear(16, 1)  # Output layer with 1 node\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.relu(self.conv1(x, edge_index))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.conv2(x, edge_index))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.conv3(x, edge_index))\n",
    "        x = self.dropout(x)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we train the network. Again, this step will take a little while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn_atten_model = GNN_atten(input_dim).to(device)\n",
    "optimizer = optim.AdamW(gnn_atten_model.parameters()\n",
    "                       , lr=0.001\n",
    "                    #    , weight_decay= weight_decay\n",
    "                       ) \n",
    "gnn2_model, train_losses, val_losses = fitGNN(gnn_atten_model, t_loader, v_loader, num_epochs, batch_size, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we make a plot for this model as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn_atten_model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with tch.no_grad():\n",
    "    for v_batch in v_loader:\n",
    "        v_batch.to(device)\n",
    "        test_outputs = gnn_atten_model(v_batch.x, v_batch.edge_index, v_batch.batch)\n",
    "        all_preds.extend(test_outputs.tolist())\n",
    "        all_labels.extend(v_batch.y.tolist())\n",
    "all_preds_tensor = tch.tensor(all_preds)\n",
    "all_labels_tensor = tch.tensor(all_labels)\n",
    "gnn_atten_mse = mean_squared_error(all_labels_tensor, all_preds_tensor)\n",
    "\n",
    "print(f'Attention GNN Regression: MSE = {gnn_atten_mse:.3f}')\n",
    "plot_history(train_losses, val_losses, 'Attention GNN Regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result\n",
    "Finally we make a plot of the results for the different methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar([\"SVM\", \"Random Forest\", \"Conv. GNN\", \"Atten. GNN\"], [SV_mse, RF_mse, gnn_conv_mse, gnn_atten_mse])\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.title(\"Results from the lab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, on the x axis we list the four models we have built and on the y axis we have MSE (so lower values are better).\n",
    "\n",
    "So, you results should be that the convolutional GNN performs the worst (probably that method is just not very good for this kind of data) and the classical machine learning methods should perform about the same as the attention based GNN. Notice that we ran just default values for SVM and RF so there is probably room for some improvement with them. With the amount of data-points in the solubility set, they train much quicker, and achieve parity with GNNs.\n",
    "\n",
    "The deep learning approach scales better with larger amount of compute, and benefits from larger amounts of data. We can see from the slope on the loss plots that the GNNs would benefit from further training as well. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
